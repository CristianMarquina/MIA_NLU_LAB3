{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2850f8b6-4428-4c96-a969-725832d54cd6",
   "metadata": {},
   "source": [
    "## Stage 2: Data Preparation Pipeline & Feature Extraction\n",
    "\n",
    "Milestone: Implementation of the oracle, feature extraction, and obtaining training samples for a Keras model.\n",
    "\n",
    "In this section, we execute the complete data processing pipeline to transform the raw CoNLL-U training data into numerical vectors that can be fed into the Neural Network. This process involves four main steps:\n",
    "\n",
    "\n",
    "Data Loading & Filtering: We load the training dataset (en_partut-ud-train.conllu) and filter out non-projective trees, as the Arc-Eager algorithm is restricted to projective dependency structures.\n",
    "\n",
    "Oracle Execution (Obtaining Samples): We run the Oracle on every valid sentence. The Oracle simulates the parsing process using the \"gold standard\" tree to generate the correct sequence of States (Input) and Transitions (Output/Target).\n",
    "\n",
    "\n",
    "Feature Extraction: We convert the complex State objects into fixed-length lists of features using the state_to_feats function. This extracts the specific words and UPOS tags from the top of the Stack and the Buffer.\n",
    "\n",
    "\n",
    "Numerical Conversion (Vectorization): Neural networks require numerical input. We build vocabularies (dictionaries mapping strings to unique Integer IDs) for words, tags, actions, and dependency labels. Finally, we convert all text features into Numpy arrays (X_train, y_act, y_dep) ready for Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "119375a6-1679-4692-a15b-b51a760802b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- STEP 1: Data Loading ---\n",
      " Loaded 1748 valid projective sentences for training.\n",
      "\n",
      "--- STEP 2: Generating Samples with the Oracle ---\n",
      "Total samples (game states) generated: 81182\n",
      "Example of Raw Sample (Index 0):\n",
      "   State: Stack (size=1): (0, ROOT, ROOT_UPOS)\n",
      "Buffer (size=13): (1, Distribution, NOUN) | (2, of, ADP) | (3, this, DET) | (4, license, NOUN) | (5, does, AUX) | (6, not, PART) | (7, create, VERB) | (8, an, DET) | (9, attorney, NOUN) | (10, -, PUNCT) | (11, client, NOUN) | (12, relationship, NOUN) | (13, ., PUNCT)\n",
      "Arcs (size=0): set()\n",
      "\n",
      "   Correct Action: SHIFT\n",
      "\n",
      "--- STEP 3: Feature Extraction (Translation to Text) ---\n",
      " Example of Input (X_raw[0]): ['<PAD>', 'ROOT', 'Distribution', 'of', '<PAD>', 'ROOT_UPOS', 'NOUN', 'ADP']\n",
      "   (This is what the network 'sees': words and tags)\n",
      "Example of Output (Y_raw[0]): ('SHIFT', None)\n",
      "   (This is what the network must predict: Action and Label)\n",
      "\n",
      "--- STEP 4: Numerical Conversion (For Keras) ---\n",
      "Vocabulary Sizes:\n",
      "   Unique words: 6872\n",
      "   Unique UPOS tags: 20\n",
      "   Possible actions: 4 {'SHIFT': 0, 'LEFT-ARC': 1, 'RIGHT-ARC': 2, 'REDUCE': 3}\n",
      "   Dependency relations: 44\n",
      "\n",
      "DATA\n",
      "Final numerical example (X_train[0]): [0 2 3 4 0 2 3 4]\n",
      "   (Notice how words are now IDs)\n",
      "Target Action (y_act[0]): 0 -> Corresponds to 'SHIFT'\n",
      "Target Action (y_act[0]): 0 -> Corresponds to 'SHIFT'\n",
      "Data saved to 'training_data.npz' and 'vocabs.pkl'\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from conllu_reader import ConlluReader\n",
    "from algorithm import ArcEager\n",
    "import pickle\n",
    "\n",
    "# --- 1. LOAD DATA (Use the TRAIN file, not test) ---\n",
    "print(\"--- STEP 1: Data Loading ---\")\n",
    "reader = ConlluReader()\n",
    "# Ensure the filename matches your specific training file path\n",
    "train_sentences = reader.read_conllu_file(\"en_partut-ud-train_clean.conllu\") \n",
    "\n",
    "# Filter out non-projective trees as Arc-Eager cannot handle them [cite: 1100]\n",
    "train_sentences = reader.remove_non_projective_trees(train_sentences)\n",
    "print(f\" Loaded {len(train_sentences)} valid projective sentences for training.\\n\")\n",
    "\n",
    "# --- 2. OBTAIN RAW SAMPLES (Oracle Execution) ---\n",
    "print(\"--- STEP 2: Generating Samples with the Oracle ---\")\n",
    "arc_eager = ArcEager()\n",
    "raw_samples = []\n",
    "\n",
    "for sent in train_sentences:\n",
    "    try:\n",
    "        # The oracle returns a list of Sample objects (State + Transition) for this sentence\n",
    "        samples = arc_eager.oracle(sent)\n",
    "        raw_samples.extend(samples)\n",
    "    except AssertionError:\n",
    "        # If the oracle fails to reconstruct the exact gold tree, skip the sentence\n",
    "        continue\n",
    "\n",
    "print(f\"Total samples (game states) generated: {len(raw_samples)}\")\n",
    "\n",
    "# VISUALIZATION: Let's see what a raw sample looks like\n",
    "if raw_samples:\n",
    "    print(f\"Example of Raw Sample (Index 0):\")\n",
    "    print(f\"   State: {raw_samples[0].state}\")\n",
    "    print(f\"   Correct Action: {raw_samples[0].transition}\\n\")\n",
    "\n",
    "# --- 3. FEATURE EXTRACTION (From State to List of Strings) ---\n",
    "# We need to extract features from the stack and buffer [cite: 934, 1080]\n",
    "print(\"--- STEP 3: Feature Extraction (Translation to Text) ---\")\n",
    "X_raw = [] # Stores lists of words/tags (Input features)\n",
    "Y_raw = [] # Stores actions and dependencies (Outputs)\n",
    "\n",
    "for sample in raw_samples:\n",
    "    # Extract features (words and UPOS tags) using the implemented function\n",
    "    # nbuffer_feats=2 and nstack_feats=2 is the suggested configuration [cite: 1091]\n",
    "    features = sample.state_to_feats(nbuffer_feats=2, nstack_feats=2)\n",
    "    X_raw.append(features)\n",
    "    \n",
    "    # Save the action (transition) and the dependency label\n",
    "    action_name = sample.transition.action\n",
    "    dep_label = sample.transition.dependency\n",
    "    Y_raw.append((action_name, dep_label))\n",
    "\n",
    "# VISUALIZATION: What do the lists contain now?\n",
    "print(f\" Example of Input (X_raw[0]): {X_raw[0]}\")\n",
    "print(f\"   (This is what the network 'sees': words and tags)\")\n",
    "print(f\"Example of Output (Y_raw[0]): {Y_raw[0]}\")\n",
    "print(f\"   (This is what the network must predict: Action and Label)\\n\")\n",
    "\n",
    "# --- 4. PREPARATION FOR KERAS (Vocabularies and Numerical Conversion) ---\n",
    "# Neural networks require numerical input [cite: 733]\n",
    "print(\"--- STEP 4: Numerical Conversion (For Keras) ---\")\n",
    "\n",
    "# 4.1 Create Dictionaries (Text -> Number Maps)\n",
    "words_vocab = {'<PAD>': 0, '<UNK>': 1}\n",
    "upos_vocab = {'<PAD>': 0, '<UNK>': 1}\n",
    "actions_vocab = {}  # E.g., 'SHIFT': 0, 'LEFT-ARC': 1...\n",
    "deprels_vocab = {None: 0} # E.g., 'nsubj': 1, 'det': 2...\n",
    "\n",
    "# Fill vocabularies by iterating through all collected data\n",
    "for features in X_raw:\n",
    "    # Assuming features structure: [W_s2, W_s1, W_b1, W_b2, P_s2, P_s1, P_b1, P_b2]\n",
    "    # The first half are words, the second half are UPOS tags\n",
    "    num_words = len(features) // 2 \n",
    "    \n",
    "    words = features[:num_words]\n",
    "    upos = features[num_words:]\n",
    "    \n",
    "    for w in words:\n",
    "        if w not in words_vocab:\n",
    "            words_vocab[w] = len(words_vocab)\n",
    "    for u in upos:\n",
    "        if u not in upos_vocab:\n",
    "            upos_vocab[u] = len(upos_vocab)\n",
    "\n",
    "for act, dep in Y_raw:\n",
    "    if act not in actions_vocab:\n",
    "        actions_vocab[act] = len(actions_vocab)\n",
    "    if dep not in deprels_vocab:\n",
    "        deprels_vocab[dep] = len(deprels_vocab)\n",
    "\n",
    "print(f\"Vocabulary Sizes:\")\n",
    "print(f\"   Unique words: {len(words_vocab)}\")\n",
    "print(f\"   Unique UPOS tags: {len(upos_vocab)}\")\n",
    "print(f\"   Possible actions: {len(actions_vocab)} {actions_vocab}\")\n",
    "print(f\"   Dependency relations: {len(deprels_vocab)}\\n\")\n",
    "\n",
    "# 4.2 Convert everything to Numbers (Matrices for Keras)\n",
    "# X_train will have shape (Num_Samples, Num_Features)\n",
    "X_train_numerical = []\n",
    "Y_train_actions = []\n",
    "Y_train_deprels = []\n",
    "\n",
    "for i in range(len(X_raw)):\n",
    "    # Convert INPUT (Features)\n",
    "    features = X_raw[i]\n",
    "    num_vec = []\n",
    "    \n",
    "    # Convert words to IDs\n",
    "    num_words = len(features) // 2\n",
    "    for w in features[:num_words]:\n",
    "        num_vec.append(words_vocab.get(w, words_vocab['<UNK>']))\n",
    "    # Convert UPOS tags to IDs\n",
    "    for u in features[num_words:]:\n",
    "        num_vec.append(upos_vocab.get(u, upos_vocab['<UNK>']))\n",
    "    \n",
    "    X_train_numerical.append(num_vec)\n",
    "    \n",
    "    # Convert OUTPUT (Targets)\n",
    "    act, dep = Y_raw[i]\n",
    "    Y_train_actions.append(actions_vocab[act])\n",
    "    # Use 0 if the dependency is None (e.g., for SHIFT or REDUCE)\n",
    "    Y_train_deprels.append(deprels_vocab.get(dep, 0)) \n",
    "\n",
    "# Convert to Numpy arrays (The actual input format Keras expects)\n",
    "X_train = np.array(X_train_numerical)\n",
    "y_act = np.array(Y_train_actions)\n",
    "y_dep = np.array(Y_train_deprels)\n",
    "\n",
    "print(\"DATA\")\n",
    "print(f\"Final numerical example (X_train[0]): {X_train[0]}\")\n",
    "print(f\"   (Notice how words are now IDs)\")\n",
    "# Find the action name corresponding to the ID for display purposes\n",
    "act_name = list(actions_vocab.keys())[list(actions_vocab.values()).index(y_act[0])]\n",
    "print(f\"Target Action (y_act[0]): {y_act[0]} -> Corresponds to '{act_name}'\")\n",
    "\n",
    "print(f\"Target Action (y_act[0]): {y_act[0]} -> Corresponds to '{act_name}'\")\n",
    "np.savez(\"training_data.npz\", X=X_train, y_act=y_act, y_dep=y_dep)\n",
    "with open(\"vocabs.pkl\", \"wb\") as f:\n",
    "    pickle.dump((words_vocab, upos_vocab, actions_vocab, deprels_vocab), f)\n",
    "print(\"Data saved to 'training_data.npz' and 'vocabs.pkl'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e6d5a0e-388c-4f55-8cc1-a5cbf3c55d12",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
